{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science on Textual Information\n",
    "> Papers on NLP, transformer, NER etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### [Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning](https://arxiv.org/abs/1801.09851)\n",
    "> Xuan et al. 2018-10-09\n",
    "\n",
    "> A paper focused on BioNER, break **14 SOTA** out of 15 NER tasks, official [source code here](https://github.com/yuzhimanhua/lm-lstm-crf)\n",
    "\n",
    "> #### Techniques:\n",
    "* **Character level** tokenization + char-level embedding + char-level LSTM (1st LSTM layer)\n",
    "* **Word level** tokenization + word-level embedding +  word-level LSTM (2nd LSTM layer)\n",
    "* Missing word tokens (**OOV**: out of vocabulary) are solved by: **the hidden state from char-level LSTM by the position of word boundaries**, concatenate with the word embedding input, then into word-level LSTM. So when word-level vocab is ```[UNK]```, the model can further deduce by the hidden state from char-level model\n",
    "* CRF **Cross Random Field** is deployed for classification, see official [pytorch implementation](https://github.com/yuzhimanhua/LM-LSTM-CRF/blob/master/model/lstm_crf.py) and```Lampel et al```\n",
    "* The multi-task problem: \n",
    "    * each task, dataset too few\n",
    "    * each task has different class labels, simple comibination will lead to many false negative prediction\n",
    "    * This paper try talcking above by **sharing weights**:\n",
    "        * The LSTM model weights are divided to $\\theta_{w},\\theta_{c},\\theta_{o}$\n",
    "        * as weights from **w**ord level, **c**har level, **o**utput layer (CRF)\n",
    "        * Experiments went through: \n",
    "            * only share $\\theta_{w}$, model MTM-W\n",
    "            * only share $\\theta_{c}$, model MTM-C\n",
    "            * share both $\\theta_{c}$ and $\\theta_{w}$, model MTM-CW (awesome one)\n",
    "            \n",
    "> #### Other mention\n",
    "* Explained the BioNER datasets from [MTL datasets](https://github.com/cambridgeltl/MTL-Bioinformatics-2016)\n",
    "* AS they all follow IOBES scheme:\n",
    "    * I: entity **i**n the middle\n",
    "    * O: n**o**t an entity\n",
    "    * B: entity by the **b**egin\n",
    "    * E: entity by the **e**nd\n",
    "    * S: **s**ingle token entity  \n",
    "* Other NER (solid old school) system\n",
    "    * CHEMDNER(Lu et al. , 2019), CRF + Brown clustering of words\n",
    "    * TaggerOne (Leaman and Lu, 2016), semi-Markov model for joint entity recognition and normalization\n",
    "    * JNLPBA, (Zhou and Su, 2004), using HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pretrain & Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)\n",
    "> This paper from Jeremy Howard et al has compared and discussed pretraining model in length. \n",
    "The paper practice pretrain and fine tuning by the following 3 steps\n",
    "* **step 1** LM for general textual material, or in related domain\n",
    "* **step 2** LM on task text\n",
    "* **step 3** Fine-tune on task text vs task labels\n",
    "\n",
    ">The paper experimented through many useful techniques like \n",
    "* Discriminated fine-tuning (step 2): different LR for different parameters, after some experiment, $\\eta^{-1} = \\eta / 2.6$ is usually good\n",
    "* Slanted triangular learning rates (step 1): which first **linearly increases** the learning rate and then **linearly decays** it.\n",
    "* Concat pooling (step 3): for hidden states $H = \\{h_{1},h_{2},...,h_{t}\\}$:\n",
    "$h_{c} = [t_{T},maxpool(H),meanpool(H)]$, where $[]$ is the concatenation\n",
    "* Gradual unfreezing (step 3), from top layer, **unfreeze 1 layer an epoch**, train all the unfreezed layers at the epoch.(use this under discriminated fine-tuning)\n",
    "* **Back propagation through time** for text classification (BPT3C) (step 3): use the output state of then end of the last sentence as the initial state of the next sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Transformer paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Background: do transduction task entirely with self-attention without using RNN\n",
    "* This paper experiments on a transduction task: machine translation\n",
    "* Learn the vector representation $z = (z_{1},...,z_{n})$ from symbol representation$x = (x_{1},...,x_{n})$, to generate output sequence $(y_{1},...,y_{m})$ with better **parallelized computation**, better than CNN and RNN, especially when $k > n$\n",
    "* The paper spent many paragraphs discuss model structure\n",
    "    * **Encoder, Decoder** stacks\n",
    "        * Encoder & Decoder consists of 6 identical layers:\n",
    "        * a layer consist of 2 **sublayer** structure, by sublayer, it's $LayerNorm(x+Sublayer(x))$, residual connection\n",
    "        * 1 sublayer is: **multi-head self-attention**, the other is **Position-wise feed forward**\n",
    "    * About the multi-head self-**Attention**: \n",
    "        * query, key & value structure, \n",
    "            * Encoder-decoder attention: Q-decoder, K,V - encoder, mimics seq2seq\n",
    "            * encoder, self-attention with Q,K,V\n",
    "            * decoder, self-attention with Q,K,V, masking out illegal connection\n",
    "        * $Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_{k}}})V$, Afther the dot product, we'll have very scaled up value if we have large ${d_k}$, which will make softmax has extremely small gradients, hence divide by $\\sqrt{d_{k}}$\n",
    "        * Why & How we practice multi-heading,eg. $d_{model} = 512$, if $h=8$, $d_{k}=d_{v}=d_{model}/h=64$\n",
    "    * Position-wise **feed forward**\n",
    "    * Positional encoding, since no RNN, no CNN, no tell of absolute and relative postion\n",
    "        * **fix** & **learning** based positional encode added to embedding output, the learning-based **works** as fine as the fix-based, but fix-based can better extrapolate\n",
    "* Self-attention generated from this model, when visualized, can show text syntax structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GPT based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### GPT paper [Improve Language Understanding by Generative Pre-Training, Radford et al.](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Use Transformer-like(only decoder) model structure with **2 stages**\n",
    "* **Un-supervised pretraining** Text generation as pretraining\n",
    "* **Supervised finetuning** Discriminative finetune on specific task (auxiliary objective)\n",
    "\n",
    "> Framework\n",
    "* Unsupervised training: language modeling, guessing the next-word, trying to maximize the following\n",
    "    * $L_{1}(u) = \\sum_{i} log P(u_{i}|u_{i-k},...,u_{i-1};\\Theta)$\n",
    "* Supervised, with extra parameter $W_{y}$ to predict $y$, $P(y|x^{1},...,x^{m}) = softmax(h_{l}^{m}W_{y})$, trying to maximize, $h_{l}^{m}$ is the last token's corresponding vector at last layer's outputs: \n",
    "    * $L_{2}(C) = \\sum_{x,y} log P(y|x^{1},...,x^{m})$\n",
    "    * Train with auxiliary objective: $L_{3}(C) = L_{2}(C)+\\lambda*L_{1}(C)$\n",
    "    * at this stage, only the $W_{y}$ and embedding for delimiter are trainable\n",
    "    \n",
    "> Data input\n",
    "* In case of fine-tuning qa and other structured input data, make the structured  data into special token separated long string of sentence\n",
    "* The paper pretrained on BooksCorpus dataset\n",
    "\n",
    "> Model\n",
    "* **Decoder** only, decoder is the transformer layer with **masked** self-attention heads, 12 layers of decoder transformer\n",
    "* Positional embeddings are **learnt** not fixed\n",
    "* BPE tokenized\n",
    "* GELU instead of relu\n",
    "* python ftfy([fix text for you](https://ftfy.readthedocs.io/en/latest/)) clean the text\n",
    "* spaCy tokenized the text\n",
    "* $\\lambda = 0.5$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### BERT Paper, [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al.](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> For GPT is undirectional, the \"most important new contribution\" of BERT is it makes the pretrain task **Bidirectional**, for the purpose of comparison with GPT, BERT share many hyper-parameters with GPT (to be specific, $BERT_{base}$), comparing the structure, GPT masked out the right-context input for each layer.\n",
    "\n",
    "> Use **encoder** instead of decoder, the bidirectional Transformer is often referred to as a \"Transformer encoder\", while the left-context-only version is referred to as \"Transformer decoder\"\n",
    "* only encoder\n",
    "* also learned positional encoding, but has additional segment embeddings (one emb for A of the pair, one for B of the pair)\n",
    "* The start of the sentence, we use ```[CLS]``` token, the corresponding token in last output of transformer will be used for fine-tune classification\n",
    "* Pretrain is slightly slower, but perform much better on GLUE benchmark\n",
    "\n",
    "> Model pretrained on 2 tasks\n",
    "* Masked Language Modeling (**MLM**), 15% random pick of word-piece masked\n",
    "* Next Sentence Prediction (is /is not?: next sentence), 50% random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALBERT paper, [ALBERT: A Lite BERT for Self-Supervised Learning of Language Representation Lan et al.](https://arxiv.org/abs/1909.11942v6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is having better NLP models as easy as having larger models?\n",
    "A configuration similar to BERT-large can has 18x fewer parameters, and trains 1.7x faster\n",
    "\n",
    "> Model Structure:\n",
    "* Seperate the **hidden layer size** vs **embedding size**, easier to grow hidden layers from the size of vocabulary embedding\n",
    "* Cross layer parameter **sharing**, more layers, not much param size growth\n",
    "\n",
    "> Method\n",
    "* SOP(sentence-order prediction) instead of NSP (next sentenc prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELECTRA paper, [ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generator, Clark et al](https://arxiv.org/abs/2003.10555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ELECTRA short for **Efficently Learning an Encoder that Classifies Token Rplacements Accurately**,The paper proposed a training scheme, which can outperform GPT, BERT, RoBERTa, with much less computation (one GPU 4 days), a bit like GAN but not doing any adversarial learning\n",
    "* The reason this works better than BERT's traing method: MLM learning has the model to learn 15% of the positions in the sentence, this doesn't seem an efficient way of learning\n",
    "\n",
    "> Method:\n",
    "* Use 2 encoder transformer, \n",
    "    * Generator, training on MLM(masked language modeling) task\n",
    "    * Discriminator,training on the generated text with guessed tokens from Generator, trying to do classification(original or replaced) on **every token** (that's how ELECTRA learns faster than usual MLM)\n",
    "* If Generator guessed the write token, it will be treated as original (different from GAN)\n",
    "* We try to minimized the combined loss $\\mathcal{L}_{MLM}(x,\\theta_{G}) +\\lambda \\mathcal{L}_{Disc(x,\\theta_{D})}$\n",
    "* Don't backpropagate the discriminator loss through the generator\n",
    "* Train the 2 model altogether from the start, then only D, never only G at first, G would be too difficult for D\n",
    "\n",
    "> Model structure\n",
    "* same as BERT-Base\n",
    "* Generators are **smaller** (decreasing layer size,1/4 to 1/2 of discriminator works the best), so can only share embedding, if same structure, all the weights can be tied\n",
    "* **Small**, designed to be trained on a single GPU\n",
    "    * Sequence length:512 to 128\n",
    "    * token embedding: 768 to 128\n",
    "    * hidden dimension size 768 to 256\n",
    "    * batch size: 256 to 128\n",
    "* **Large** , same as BART large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
