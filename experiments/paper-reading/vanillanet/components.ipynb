{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanillaNet Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers import DropPath, trunc_normal_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation\n",
    "> Series Informed Activation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusing_conv_and_bn(kernel, bias, gamma, beta, running_mean, running_var, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    Combine the convolutional kernel and the batch normalization parameters.\n",
    "    besides eps, all parameters are learnable parameters.\n",
    "\n",
    "    kernel, bias are from the convolutional layer.\n",
    "    gamma, beta, running_mean, running_var are from the batch normalization layer.\n",
    "\n",
    "    The output is a new kernel and a new bias, which can be used to replace the original convolutional layer.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta + (bias - running_mean) * gamma / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Series informed activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, act_num: int = 3, inference: bool = False, activation_module: nn.Module = nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.act_num = act_num\n",
    "        self.inference = inference\n",
    "\n",
    "        # The weight is in size of (channel_in, 1, kernel_size, kernel_size)\n",
    "        self.weight = nn.Parameter(torch.randn(dim, 1, act_num*2+1, act_num*2+1))\n",
    "        self.activation_module = activation_module\n",
    "        \n",
    "        if inference:\n",
    "            self.bias = nn.Parameter(torch.zeros(dim))\n",
    "        else:\n",
    "            self.bn = nn.BatchNorm2d(dim, eps=1e-6)\n",
    "\n",
    "        trunc_normal_(self.weight, std=.02)\n",
    "\n",
    "    def _fuse_bn_tensor(self, weight: nn.Module, bn: nn.Module):\n",
    "        \"\"\"\n",
    "        Fuse the convolutional layer weights and the batchnorm layer into one layer.\n",
    "        \"\"\"\n",
    "        new_kernel, new_bias = fusing_conv_and_bn(\n",
    "            kernel=weight,\n",
    "            bias=0, # we don't have bias before combine\n",
    "            gamma=bn.weight,\n",
    "            beta=bn.bias,\n",
    "            running_mean=bn.running_mean,\n",
    "            running_var=bn.running_var, eps=bn.eps)\n",
    "        return new_kernel, new_bias\n",
    "        \n",
    "    def switch_to_inference(self):\n",
    "        if self.inference:\n",
    "            print(\"already in inference mode\")\n",
    "            return\n",
    "        kernel, bias = self._fuse_bn_tensor(self.weight, self.bn)\n",
    "\n",
    "        self.weight.data = kernel\n",
    "        self.bias = nn.Parameter(bias)\n",
    "        self.bn.to('cpu')\n",
    "        self.__delattr__('bn')\n",
    "        self.inference = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.activation_module(x)\n",
    "        if self.inference:\n",
    "            return F.conv2d(x, self.weight, bias=self.bias, padding=self.act_num,\n",
    "                groups=self.dim, #number of input channels\n",
    "            )\n",
    "        else:\n",
    "            return self.bn(\n",
    "                F.conv2d(x, self.weight, padding=self.act_num,\n",
    "                    groups=self.dim, #number of input channels\n",
    "                ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_leaky = SIActivation(64, 3, False, nn.LeakyReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 64, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is inference: False\n",
      "is inference: True\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(f\"is inference: {si_leaky.inference}\")\n",
    "    y_1 = si_leaky(x.clone())\n",
    "\n",
    "si_leaky.switch_to_inference()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"is inference: {si_leaky.inference}\")\n",
    "    y_2 = si_leaky(x.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5920, -0.1833,  0.2094,  ...,  1.1171,  1.1279, -0.2087],\n",
       "          [ 0.2836, -0.8651,  0.1697,  ...,  0.5286,  0.3792,  0.3336],\n",
       "          [ 1.3090, -0.4084,  0.0641,  ...,  0.6949,  1.4974,  0.8812],\n",
       "          ...,\n",
       "          [-0.4187, -1.1463, -0.2624,  ...,  0.2385, -0.6644,  0.9209],\n",
       "          [ 0.0413,  0.1789, -1.1674,  ..., -0.5877,  0.1802, -0.5983],\n",
       "          [ 0.2251, -0.6933, -0.5823,  ..., -0.9055, -0.1351,  0.3962]],\n",
       "\n",
       "         [[-0.8529, -0.8245, -0.3242,  ..., -0.9185, -0.9937, -0.7371],\n",
       "          [-0.3071, -0.0938, -0.4368,  ..., -0.2782, -0.9000, -0.9356],\n",
       "          [-1.0432, -0.3383, -0.9051,  ..., -0.0265, -0.9231,  0.1820],\n",
       "          ...,\n",
       "          [-1.5265,  0.8418, -0.0559,  ...,  0.1747,  0.4012,  0.0962],\n",
       "          [-0.1305,  0.0170,  0.4680,  ...,  0.4032,  0.8075,  1.1066],\n",
       "          [ 0.6469,  0.3358,  0.9091,  ..., -0.1819,  0.6818,  0.2476]],\n",
       "\n",
       "         [[-0.1031,  0.7024, -1.0838,  ..., -0.8862,  1.0748,  0.7174],\n",
       "          [-0.5285, -0.2928, -1.2260,  ...,  0.7603,  1.1077,  0.2544],\n",
       "          [-0.0582, -0.2999,  0.3097,  ...,  1.4004, -0.5461,  1.4282],\n",
       "          ...,\n",
       "          [ 0.2178, -1.1053,  1.3588,  ..., -0.2463,  0.5890,  2.2315],\n",
       "          [ 0.1637,  0.4332, -0.0722,  ...,  0.4253, -0.2715,  0.5526],\n",
       "          [ 0.7074,  0.6844,  0.7347,  ...,  1.3610,  0.5871,  0.4325]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2154,  0.2471,  0.3462,  ...,  0.4334, -0.1964,  0.2900],\n",
       "          [-0.1574, -0.6794, -0.0970,  ..., -2.1289,  0.5048, -0.9136],\n",
       "          [ 0.3465,  0.2820, -0.2622,  ...,  0.5911, -0.3019, -0.7136],\n",
       "          ...,\n",
       "          [-0.2601,  0.2875,  0.1997,  ..., -1.2665, -0.3318, -0.6553],\n",
       "          [ 0.1811, -0.7587,  0.8766,  ...,  1.7622, -0.2021,  0.3881],\n",
       "          [-0.2001,  0.4712,  0.6698,  ...,  0.6617, -1.6113,  0.3397]],\n",
       "\n",
       "         [[-0.3891,  0.6813, -0.7049,  ..., -0.4857,  0.4902, -0.5992],\n",
       "          [-0.5547,  0.8891, -0.2317,  ..., -1.0559,  0.4768, -0.6004],\n",
       "          [-0.0139, -1.3581,  1.0654,  ..., -0.7074, -0.2151, -0.2375],\n",
       "          ...,\n",
       "          [ 0.4695, -0.3134, -0.7436,  ..., -1.7491, -0.8801,  0.5421],\n",
       "          [ 0.4194,  1.7881, -0.9622,  ...,  0.2595, -0.8117, -0.1468],\n",
       "          [ 0.1838, -1.1465, -0.3612,  ..., -0.5085,  0.1821,  0.0669]],\n",
       "\n",
       "         [[ 0.9032,  0.6542,  0.5775,  ...,  1.4051,  2.3787,  1.6466],\n",
       "          [-0.1223,  0.0139, -0.2704,  ...,  0.6212,  0.5317,  1.7103],\n",
       "          [ 0.4049,  1.1825, -0.0724,  ...,  0.2497,  0.7842,  0.2229],\n",
       "          ...,\n",
       "          [ 0.3364,  0.4011,  0.2454,  ...,  0.8671, -0.0311,  1.7583],\n",
       "          [ 0.1992,  1.0428, -0.3606,  ...,  1.5902,  0.8200,  1.2346],\n",
       "          [ 1.4230,  0.1943,  0.1919,  ...,  0.0451,  0.6804,  0.1251]]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_1 - y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 64, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(64, 64, 3, padding=1).weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn  = nn.BatchNorm2d(64, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.weight, bn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.bias, bn.bias.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Under usual batch normalization the following is computed and updated, notice it involves 4 players (learnable parameters): $\\gamma, \\beta, \\mu, \\sigma^2$\n",
    "\n",
    "#### Calculation & Update\n",
    "\n",
    "Mean of the batch, easy to compute\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "$\n",
    "\n",
    "The following is basically standard deviation of the batch\n",
    "\n",
    "$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "$\n",
    "\n",
    "Normalize the batch, shift the distribution to zero mean and unit variance\n",
    "\n",
    "$\n",
    "\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$\n",
    "\n",
    "Running mean and variance update (using mmentum to decay the running mean and variance)\n",
    "\n",
    "$\n",
    "\\mu = \\alpha \\mu + (1 - \\alpha) \\mu_B$\n",
    "\n",
    "$\n",
    "\\sigma^2 = \\alpha \\sigma^2 + (1 - \\alpha) \\sigma_B^2\n",
    "$\n",
    "\n",
    "Scale and shift the normalized batch\n",
    "\n",
    "$\n",
    "y_i = \\gamma \\hat{x_i} + \\beta\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-8.9798e-01,  9.2157e-01, -9.0759e-01,  ...,  5.8390e-01,\n",
       "            6.2553e-01,  3.3800e-01],\n",
       "          [-4.8013e-01,  3.0036e-01, -4.2789e-01,  ...,  2.6584e-01,\n",
       "            1.3161e+00,  2.2439e-01],\n",
       "          [ 6.3771e-01, -1.6585e+00,  1.7861e-01,  ...,  2.4691e+00,\n",
       "           -8.1982e-01, -1.3654e-01],\n",
       "          ...,\n",
       "          [ 1.1954e+00, -8.5129e-01, -5.4851e-01,  ..., -9.7521e-01,\n",
       "           -1.1201e+00,  1.7426e-01],\n",
       "          [-2.1337e-01,  6.2733e-01, -6.7771e-01,  ..., -9.2922e-01,\n",
       "           -2.6842e+00,  1.1628e-01],\n",
       "          [ 1.9230e-01, -2.1457e+00, -1.4950e-01,  ...,  4.6629e-01,\n",
       "           -1.9567e+00, -6.1159e-02]],\n",
       "\n",
       "         [[ 1.2826e+00, -6.5572e-01, -1.1191e-01,  ...,  1.3191e+00,\n",
       "            3.3968e-01, -9.8841e-01],\n",
       "          [ 1.7222e-01,  3.9568e-01, -8.2014e-01,  ...,  1.3924e+00,\n",
       "           -1.1021e+00, -1.4651e+00],\n",
       "          [-9.3531e-01,  6.5933e-01,  8.7128e-02,  ..., -1.1898e+00,\n",
       "           -1.5951e+00,  1.3119e+00],\n",
       "          ...,\n",
       "          [-2.6904e-01,  1.4871e-01, -5.5069e-01,  ...,  5.2512e-01,\n",
       "            1.2293e+00,  4.0446e-02],\n",
       "          [ 5.3979e-01, -6.5122e-01,  1.1335e+00,  ..., -2.1051e+00,\n",
       "           -7.8902e-01, -1.5158e+00],\n",
       "          [ 2.2303e-01,  1.0582e-02, -2.9026e-01,  ..., -1.2727e+00,\n",
       "           -2.8300e-01,  3.3349e-01]],\n",
       "\n",
       "         [[-7.0131e-01,  9.7813e-01,  1.2305e+00,  ..., -2.3734e+00,\n",
       "           -2.5653e-01, -9.3386e-02],\n",
       "          [-9.6503e-01, -3.7535e-02, -1.8183e-01,  ..., -4.3351e-01,\n",
       "            8.7881e-01, -4.6009e-01],\n",
       "          [ 4.1779e-01,  9.3556e-01, -3.4015e-01,  ..., -1.0939e+00,\n",
       "           -3.1543e-01, -2.6794e-01],\n",
       "          ...,\n",
       "          [-3.8393e-01, -9.8269e-01, -5.9674e-02,  ..., -7.2696e-01,\n",
       "            9.8109e-01, -4.0319e-01],\n",
       "          [ 3.5118e-01,  1.3035e+00, -6.1814e-01,  ..., -6.9294e-02,\n",
       "            1.0089e+00, -4.8616e-01],\n",
       "          [ 5.6628e-01,  4.7035e-01,  4.3796e-01,  ...,  1.0189e+00,\n",
       "            5.0908e-01,  2.5973e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3198e+00,  1.7258e+00, -1.1856e+00,  ...,  4.9860e-01,\n",
       "           -3.1654e-01, -5.1618e-01],\n",
       "          [ 8.9593e-01,  3.2726e-01, -7.6662e-01,  ..., -6.3983e-01,\n",
       "            4.2062e-01, -7.7337e-01],\n",
       "          [ 1.5718e-01,  4.3225e-01, -7.3734e-01,  ..., -7.4405e-02,\n",
       "            1.5917e+00,  5.2954e-01],\n",
       "          ...,\n",
       "          [ 8.4115e-01,  9.1206e-01, -1.0015e+00,  ...,  7.5438e-01,\n",
       "           -3.4053e-01,  9.4340e-01],\n",
       "          [-9.3419e-01,  2.0302e-01, -1.1233e+00,  ..., -1.8578e+00,\n",
       "           -1.2822e+00, -9.2166e-01],\n",
       "          [-8.0459e-01,  7.5160e-01, -1.6338e+00,  ..., -1.5329e+00,\n",
       "            2.1282e+00, -6.7997e-01]],\n",
       "\n",
       "         [[-4.3562e-01,  1.5525e-01,  8.2253e-01,  ...,  1.4644e+00,\n",
       "            7.6464e-02, -1.5558e+00],\n",
       "          [ 7.0608e-01, -1.2122e+00, -1.3212e-01,  ..., -1.1682e+00,\n",
       "            6.9843e-01, -3.6046e-01],\n",
       "          [-3.5467e-01,  5.2223e-01,  1.4963e+00,  ..., -4.8367e-01,\n",
       "            8.9255e-02,  1.8985e+00],\n",
       "          ...,\n",
       "          [-7.2146e-01,  1.6233e+00,  8.6496e-02,  ...,  1.0797e+00,\n",
       "            3.8164e-01,  3.8860e-01],\n",
       "          [-2.6919e-01,  1.1461e+00, -1.6623e+00,  ..., -1.0043e+00,\n",
       "           -1.7314e+00, -4.7943e-02],\n",
       "          [-6.8233e-01,  2.6212e-01,  7.7520e-01,  ..., -3.7255e-01,\n",
       "            1.9470e+00, -1.2828e-01]],\n",
       "\n",
       "         [[ 4.8715e-01,  1.2093e+00, -8.7633e-01,  ..., -6.8259e-01,\n",
       "            9.5098e-01, -1.2838e+00],\n",
       "          [ 1.1584e+00, -2.1106e-01, -6.0035e-01,  ...,  1.3111e+00,\n",
       "            1.0487e+00,  3.1041e-01],\n",
       "          [ 1.3980e+00,  6.6028e-01, -1.9123e+00,  ...,  1.1244e+00,\n",
       "            5.7842e-01, -1.0610e+00],\n",
       "          ...,\n",
       "          [ 3.1155e-01, -8.2408e-01,  1.6909e-02,  ...,  8.9257e-01,\n",
       "            6.0710e-01,  3.7673e-01],\n",
       "          [ 7.2180e-01, -6.8857e-01,  7.2458e-01,  ..., -6.0362e-01,\n",
       "           -3.9317e-01,  9.8216e-01],\n",
       "          [-5.0006e-01,  1.3079e-01,  2.8640e-01,  ..., -7.4009e-01,\n",
       "           -1.1900e+00,  9.7168e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.9425e-01, -2.1766e+00,  3.3353e+00,  ..., -5.3515e-01,\n",
       "            1.4167e-01,  2.0662e+00],\n",
       "          [-4.9149e-01,  4.8489e-01, -1.4546e+00,  ...,  1.8155e+00,\n",
       "           -5.9097e-01,  6.5186e-01],\n",
       "          [-8.8611e-01, -7.5709e-01, -1.2646e+00,  ...,  6.4897e-01,\n",
       "           -1.9281e-01, -7.2912e-01],\n",
       "          ...,\n",
       "          [ 4.8801e-01,  2.0248e-01, -1.8597e+00,  ..., -1.4360e+00,\n",
       "            3.5296e-01, -6.3087e-01],\n",
       "          [ 8.1290e-01, -2.1438e-01,  1.4939e+00,  ...,  5.4296e-01,\n",
       "           -7.8945e-01, -1.8475e+00],\n",
       "          [-3.9527e-01, -9.6540e-01,  2.1727e-01,  ..., -3.0911e+00,\n",
       "            6.5005e-01, -2.8816e-01]],\n",
       "\n",
       "         [[-2.3188e-01,  6.6831e-01, -1.2388e-01,  ..., -1.6023e-01,\n",
       "            1.7433e+00,  1.8678e+00],\n",
       "          [-8.0881e-01,  3.0794e-01,  2.8561e-01,  ...,  4.1233e-01,\n",
       "            2.8127e-01, -2.6156e+00],\n",
       "          [ 6.1845e-01, -6.4989e-01,  9.3192e-01,  ..., -2.9523e-01,\n",
       "           -5.6232e-01,  3.5109e-01],\n",
       "          ...,\n",
       "          [-9.1399e-01,  1.7693e-01,  7.8312e-01,  ..., -1.3271e+00,\n",
       "           -2.9484e-01,  1.6293e+00],\n",
       "          [ 1.6986e+00,  6.0373e-01, -3.3773e-01,  ..., -8.1290e-01,\n",
       "           -1.7949e+00, -2.4790e-01],\n",
       "          [-1.0459e+00, -8.9906e-01,  1.2430e+00,  ..., -1.5252e+00,\n",
       "           -1.7850e+00, -9.8114e-01]],\n",
       "\n",
       "         [[-1.2585e+00,  1.1783e+00,  1.2420e+00,  ...,  6.5616e-02,\n",
       "            2.5496e+00,  6.7524e-01],\n",
       "          [-2.1191e+00, -1.4353e+00,  5.1589e-01,  ...,  7.1720e-02,\n",
       "            1.4060e-01, -1.0448e-02],\n",
       "          [-8.9069e-02,  2.7957e-02,  5.4044e-01,  ...,  8.1901e-01,\n",
       "           -1.3211e+00, -1.1542e-01],\n",
       "          ...,\n",
       "          [ 1.6823e+00, -2.9167e-02, -2.2474e-01,  ..., -5.3622e-01,\n",
       "            1.3778e+00,  9.8454e-01],\n",
       "          [-3.1794e-01,  5.6556e-02,  1.5712e+00,  ...,  1.5840e-01,\n",
       "           -3.4241e-01, -1.6913e+00],\n",
       "          [ 1.1068e+00, -1.3616e+00, -1.5885e-01,  ...,  4.7291e-01,\n",
       "           -3.1810e-02,  3.0329e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.8750e-03,  1.0270e+00, -2.9062e-01,  ..., -1.5015e+00,\n",
       "            1.2899e+00, -3.6005e-01],\n",
       "          [-1.2246e+00,  1.6543e+00, -6.0680e-01,  ..., -2.2903e+00,\n",
       "            7.9837e-01,  1.7717e-01],\n",
       "          [-2.0580e+00,  7.7066e-01, -2.8646e-01,  ...,  3.4191e-01,\n",
       "           -7.6844e-01,  7.7530e-01],\n",
       "          ...,\n",
       "          [-2.1980e-01, -2.9305e-01, -1.5010e-01,  ...,  2.0179e-01,\n",
       "           -1.0964e+00, -1.9046e-01],\n",
       "          [-9.0944e-01,  6.5684e-01, -1.2377e-01,  ...,  1.0525e+00,\n",
       "           -2.5462e+00, -1.2059e+00],\n",
       "          [-3.6370e-01,  3.5258e-01, -5.2772e-01,  ..., -5.2759e-01,\n",
       "            4.9749e-01, -1.0140e-01]],\n",
       "\n",
       "         [[ 8.0281e-01, -8.0017e-01,  1.1153e+00,  ...,  6.6537e-01,\n",
       "            1.7597e-01, -9.2621e-01],\n",
       "          [-5.6339e-02, -1.2801e+00, -5.4427e-01,  ...,  2.8349e-01,\n",
       "            2.5251e-01, -7.0522e-01],\n",
       "          [-1.9252e-01,  1.3716e+00,  6.5617e-01,  ..., -6.6215e-01,\n",
       "            1.3043e+00,  2.8204e-01],\n",
       "          ...,\n",
       "          [ 8.4503e-01,  4.2378e-01,  6.0861e-01,  ...,  8.3792e-01,\n",
       "           -4.3990e-01,  1.1125e+00],\n",
       "          [-1.4858e-01,  2.7196e+00, -9.7826e-01,  ..., -1.3595e+00,\n",
       "           -8.1543e-01,  2.8528e+00],\n",
       "          [ 1.3911e+00,  5.7921e-01, -8.3303e-02,  ...,  2.1582e-04,\n",
       "           -8.2410e-01,  4.7922e-02]],\n",
       "\n",
       "         [[-1.2728e+00,  1.6475e+00,  5.8188e-01,  ..., -1.2408e+00,\n",
       "           -6.0299e-01,  1.0480e+00],\n",
       "          [ 4.3754e-01, -1.0647e+00, -1.0360e-01,  ...,  2.3358e+00,\n",
       "            1.0839e+00, -3.9121e-01],\n",
       "          [-8.5928e-01,  1.7989e+00,  4.0919e-01,  ..., -3.4922e-01,\n",
       "           -3.1329e-02,  2.2129e-01],\n",
       "          ...,\n",
       "          [ 3.8807e-02,  6.7749e-01,  7.9787e-01,  ..., -7.6752e-01,\n",
       "           -6.6348e-01,  4.1683e-01],\n",
       "          [-9.9127e-01, -1.0939e+00,  2.5718e-01,  ...,  5.5431e-01,\n",
       "           -2.9350e-01,  3.7658e-01],\n",
       "          [-7.2322e-01,  7.4359e-02, -1.7453e+00,  ...,  1.9609e+00,\n",
       "           -5.4533e-01, -1.3299e-01]]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = bn(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.1879e-10, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0000, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.6744e-04,  5.1428e-04,  4.7100e-04, -8.5480e-05, -1.2897e-04,\n",
       "          2.7630e-04, -1.9318e-04,  2.5398e-05,  4.5987e-04,  6.8352e-04,\n",
       "          6.1441e-05, -3.0484e-04, -1.5357e-04,  2.7439e-04,  1.8183e-04,\n",
       "         -6.1147e-04, -5.4306e-05, -8.7659e-05, -6.8270e-04, -2.1659e-04,\n",
       "          3.6032e-05,  5.3290e-05, -2.2876e-04, -2.5380e-05, -2.3229e-04,\n",
       "         -1.1275e-05, -3.1075e-05,  6.5662e-05,  3.9315e-05,  3.7166e-04,\n",
       "         -5.5835e-04, -1.9106e-04, -9.2222e-06,  2.0686e-04, -3.1161e-04,\n",
       "         -1.9846e-05, -3.0557e-04, -1.1270e-04,  2.1897e-04, -2.0205e-04,\n",
       "          2.1866e-04,  8.5770e-05, -1.2760e-04, -6.1896e-04, -2.0271e-04,\n",
       "         -3.5679e-05, -2.4435e-04,  3.9585e-04, -2.7237e-04, -3.5685e-04,\n",
       "         -3.3539e-04,  1.8854e-04,  1.9764e-04, -8.5256e-05,  3.4071e-04,\n",
       "          2.7728e-04,  9.3718e-05, -4.8139e-04,  5.0605e-05, -5.1907e-04,\n",
       "         -1.5690e-04, -2.1815e-04, -3.3568e-04,  1.0267e-04]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.running_mean, bn.running_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           BatchNorm2d\n",
      "\u001b[0;31mString form:\u001b[0m    BatchNorm2d(64, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BatchNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\u001b[0m\n",
      "\u001b[0;34m    with additional channel dimension) as described in the paper\u001b[0m\n",
      "\u001b[0;34m    `Batch Normalization: Accelerating Deep Network Training by Reducing\u001b[0m\n",
      "\u001b[0;34m    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    .. math::\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The mean and standard-deviation are calculated per-dimension over\u001b[0m\n",
      "\u001b[0;34m    the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\u001b[0m\n",
      "\u001b[0;34m    of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\u001b[0m\n",
      "\u001b[0;34m    to 1 and the elements of :math:`\\beta` are set to 0. The standard-deviation is calculated\u001b[0m\n",
      "\u001b[0;34m    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Also by default, during training this layer keeps running estimates of its\u001b[0m\n",
      "\u001b[0;34m    computed mean and variance, which are then used for normalization during\u001b[0m\n",
      "\u001b[0;34m    evaluation. The running estimates are kept with a default :attr:`momentum`\u001b[0m\n",
      "\u001b[0;34m    of 0.1.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    If :attr:`track_running_stats` is set to ``False``, this layer then does not\u001b[0m\n",
      "\u001b[0;34m    keep running estimates, and batch statistics are instead used during\u001b[0m\n",
      "\u001b[0;34m    evaluation time as well.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    .. note::\u001b[0m\n",
      "\u001b[0;34m        This :attr:`momentum` argument is different from one used in optimizer\u001b[0m\n",
      "\u001b[0;34m        classes and the conventional notion of momentum. Mathematically, the\u001b[0m\n",
      "\u001b[0;34m        update rule for running statistics here is\u001b[0m\n",
      "\u001b[0;34m        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\u001b[0m\n",
      "\u001b[0;34m        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\u001b[0m\n",
      "\u001b[0;34m        new observed value.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Because the Batch Normalization is done over the `C` dimension, computing statistics\u001b[0m\n",
      "\u001b[0;34m    on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        num_features: :math:`C` from an expected input of size\u001b[0m\n",
      "\u001b[0;34m            :math:`(N, C, H, W)`\u001b[0m\n",
      "\u001b[0;34m        eps: a value added to the denominator for numerical stability.\u001b[0m\n",
      "\u001b[0;34m            Default: 1e-5\u001b[0m\n",
      "\u001b[0;34m        momentum: the value used for the running_mean and running_var\u001b[0m\n",
      "\u001b[0;34m            computation. Can be set to ``None`` for cumulative moving average\u001b[0m\n",
      "\u001b[0;34m            (i.e. simple average). Default: 0.1\u001b[0m\n",
      "\u001b[0;34m        affine: a boolean value that when set to ``True``, this module has\u001b[0m\n",
      "\u001b[0;34m            learnable affine parameters. Default: ``True``\u001b[0m\n",
      "\u001b[0;34m        track_running_stats: a boolean value that when set to ``True``, this\u001b[0m\n",
      "\u001b[0;34m            module tracks the running mean and variance, and when set to ``False``,\u001b[0m\n",
      "\u001b[0;34m            this module does not track such statistics, and initializes statistics\u001b[0m\n",
      "\u001b[0;34m            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\u001b[0m\n",
      "\u001b[0;34m            When these buffers are ``None``, this module always uses batch statistics.\u001b[0m\n",
      "\u001b[0;34m            in both training and eval modes. Default: ``True``\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Shape:\u001b[0m\n",
      "\u001b[0;34m        - Input: :math:`(N, C, H, W)`\u001b[0m\n",
      "\u001b[0;34m        - Output: :math:`(N, C, H, W)` (same shape as input)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples::\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> # With Learnable Parameters\u001b[0m\n",
      "\u001b[0;34m        >>> m = nn.BatchNorm2d(100)\u001b[0m\n",
      "\u001b[0;34m        >>> # Without Learnable Parameters\u001b[0m\n",
      "\u001b[0;34m        >>> m = nn.BatchNorm2d(100, affine=False)\u001b[0m\n",
      "\u001b[0;34m        >>> input = torch.randn(20, 100, 35, 45)\u001b[0m\n",
      "\u001b[0;34m        >>> output = m(input)\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected 4D input (got {}D input)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n"
     ]
    }
   ],
   "source": [
    "bn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0052,  0.0118,  0.0075,  0.0313, -0.0141, -0.0022,  0.0135,  0.0336,\n",
       "        -0.0182,  0.0095,  0.0362, -0.0291, -0.0096,  0.0136,  0.0333,  0.0364,\n",
       "        -0.0148,  0.0255, -0.0406,  0.0132,  0.0178, -0.0404, -0.0210,  0.0030,\n",
       "         0.0086,  0.0182,  0.0111,  0.0347,  0.0068, -0.0010,  0.0102, -0.0194,\n",
       "         0.0312, -0.0225, -0.0306, -0.0191,  0.0356,  0.0318, -0.0408, -0.0364,\n",
       "        -0.0085,  0.0140, -0.0076, -0.0202, -0.0031, -0.0083, -0.0202,  0.0381,\n",
       "         0.0029,  0.0084, -0.0041, -0.0227,  0.0076,  0.0283,  0.0096, -0.0325,\n",
       "        -0.0016, -0.0215,  0.0315,  0.0345,  0.0133, -0.0228, -0.0092,  0.0322],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(64, 64, 3, padding=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
