{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Rals-C-SAGAN\n",
    "* Ra - Relativistic Average;\n",
    "* Ls - Least Squares;\n",
    "* C - Conditional;\n",
    "* SA - Self-Attention;\n",
    "* DCGAN - Deep Convolutional Generative Adversarial Network\n",
    "\n",
    "<br>\n",
    "References:\n",
    "* https://www.kaggle.com/speedwagon/ralsgan-dogs\n",
    "* https://www.kaggle.com/cdeotte/dog-breed-cgan\n",
    "* https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py\n",
    "* https://github.com/voletiv/self-attention-GAN-pytorch/blob/master/sagan_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip, pickle\n",
    "import tensorflow as tf\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "import urllib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "kernel_start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def snconv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "    return spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                   stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n",
    "\n",
    "def snlinear(in_features, out_features):\n",
    "    return spectral_norm(nn.Linear(in_features=in_features, out_features=out_features))\n",
    "\n",
    "def sn_embedding(num_embeddings, embedding_dim):\n",
    "    return spectral_norm(nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim))\n",
    "\n",
    "\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(Self_Attn, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.snconv1x1_theta = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_phi = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_g = snconv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_attn = snconv2d(in_channels=in_channels//2, out_channels=in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2, padding=0)\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        self.sigma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, ch, h, w = x.size()\n",
    "        # Theta path\n",
    "        theta = self.snconv1x1_theta(x)\n",
    "        theta = theta.view(-1, ch//8, h*w)\n",
    "        # Phi path\n",
    "        phi = self.snconv1x1_phi(x)\n",
    "        phi = self.maxpool(phi)\n",
    "        phi = phi.view(-1, ch//8, h*w//4)\n",
    "        # Attn map\n",
    "        attn = torch.bmm(theta.permute(0, 2, 1), phi)\n",
    "        attn = self.softmax(attn)\n",
    "        # g path\n",
    "        g = self.snconv1x1_g(x)\n",
    "        g = self.maxpool(g)\n",
    "        g = g.view(-1, ch//2, h*w//4)\n",
    "        # Attn_g\n",
    "        attn_g = torch.bmm(g, attn.permute(0, 2, 1))\n",
    "        attn_g = attn_g.view(-1, ch//2, h, w)\n",
    "        attn_g = self.snconv1x1_attn(attn_g)\n",
    "        # Out\n",
    "        out = x + self.sigma * attn_g\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.bn = nn.BatchNorm2d(num_features)\n",
    "        self.embed = nn.Embedding(num_classes, num_features * 2)\n",
    "        self.embed.weight.data[:, :num_features].fill_(1.)  # Initialize scale to 1\n",
    "        self.embed.weight.data[:, num_features:].zero_()    # Initialize bias at 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, y = inputs\n",
    "        \n",
    "        out = self.bn(x)\n",
    "        gamma, beta = self.embed(y).chunk(2, 1)\n",
    "        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceInBatch(x):\n",
    "    bs = x.size(0)\n",
    "    return -(x.unsqueeze(0).repeat([bs,1,1])-x.unsqueeze(-1).repeat([1,1,bs]).permute(0,2,1)).sum(1)\n",
    "\n",
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_output, num_classes, k_size=4, stride=2, padding=0, \n",
    "                 bias=False, dropout_p=0.0, use_cbn=True):\n",
    "        super(UpConvBlock, self).__init__()\n",
    "        self.use_cbn = use_cbn\n",
    "        self.dropout_p=dropout_p\n",
    "        self.upconv = spectral_norm(nn.ConvTranspose2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=bias))\n",
    "        if use_cbn: self.cond_bn = ConditionalBatchNorm2d(n_output, num_classes)\n",
    "        else:       self.bn = nn.BatchNorm2d(n_output)\n",
    "        self.activ = nn.LeakyReLU(0.05, inplace=True)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_p)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x0, labels = inputs\n",
    "        \n",
    "        x = self.upconv(x0)\n",
    "        if self.use_cbn: x = self.activ(self.cond_bn((x, labels)))\n",
    "        else:            x = self.activ(self.bn(x))\n",
    "        if self.dropout_p > 0.0: \n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=128, num_classes=120, channels=3, nfilt=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nz = nz\n",
    "        self.num_classes = num_classes\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, nz)\n",
    "        self.upconv1 = UpConvBlock(2*nz, nfilt*16, num_classes, k_size=4, stride=1, padding=0, dropout_p=0.1)\n",
    "        self.upconv2 = UpConvBlock(nfilt*16, nfilt*8, num_classes, k_size=4, stride=2, padding=1, dropout_p=0.1)\n",
    "        self.upconv3 = UpConvBlock(nfilt*8, nfilt*4, num_classes, k_size=4, stride=2, padding=1, dropout_p=0.05)\n",
    "        self.upconv4 = UpConvBlock(nfilt*4, nfilt*2, num_classes, k_size=4, stride=2, padding=1, dropout_p=0.05)\n",
    "        self.upconv5 = UpConvBlock(nfilt*2, nfilt, num_classes, k_size=4, stride=2, padding=1, dropout_p=0.05)\n",
    "        self.self_attn = Self_Attn(nfilt)\n",
    "        self.upconv6 = UpConvBlock(nfilt, 3, num_classes, k_size=3, stride=1, padding=1, use_cbn=False)\n",
    "        self.out_conv = spectral_norm(nn.Conv2d(3, 3, 3, 1, 1, bias=False))\n",
    "        self.out_activ = nn.Tanh()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        z, labels = inputs\n",
    "        \n",
    "        enc = self.label_emb(labels).view((-1, self.nz, 1, 1))\n",
    "        enc = F.normalize(enc, p=2, dim=1)\n",
    "        x = torch.cat((z, enc), 1)\n",
    "        \n",
    "        x = self.upconv1((x, labels))\n",
    "        x = self.upconv2((x, labels))\n",
    "        x = self.upconv3((x, labels))\n",
    "        x = self.upconv4((x, labels))\n",
    "        x = self.upconv5((x, labels))\n",
    "        x = self.self_attn(x)\n",
    "        x = self.upconv6((x, labels))\n",
    "        x = self.out_conv(x)\n",
    "        img = self.out_activ(x)              \n",
    "        return img\n",
    "    \n",
    "    \n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, num_classes=120, channels=3, nfilt=64):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.channels = channels\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         def down_convlayer(n_input, n_output, k_size=4, stride=2, padding=0, dropout_p=0.0):\n",
    "#             block = [spectral_norm(nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)),\n",
    "#                      nn.BatchNorm2d(n_output),\n",
    "#                      nn.LeakyReLU(0.2, inplace=True),\n",
    "#                     ]\n",
    "#             if dropout_p > 0.0: block.append(nn.Dropout(p=dropout_p))\n",
    "#             return block\n",
    "        \n",
    "#         self.label_emb = nn.Embedding(num_classes, 64*64)\n",
    "#         self.model = nn.Sequential(\n",
    "#             *down_convlayer(self.channels + 1, nfilt, k_size = 4, stride = 2, padding = 1),\n",
    "#             # output (bs,64,32,32)\n",
    "#             Self_Attn(nfilt),\n",
    "            \n",
    "#             *down_convlayer(nfilt, nfilt*2, k_size = 4, stride = 2, padding = 1, dropout_p=0.05),\n",
    "#             # output (bs,128,16,16)\n",
    "#             *down_convlayer(nfilt*2, nfilt*4, k_size = 4, stride = 2, padding = 1, dropout_p=0.10),\n",
    "#             # output (bs,256,8,8)\n",
    "#             *down_convlayer(nfilt*4, nfilt*8, k_size = 4, stride = 2, padding = 1, dropout_p=0.15),\n",
    "#             # output (bs,512,4,4)\n",
    "    \n",
    "#             spectral_norm(nn.Conv2d(nfilt*8, out_channels = 1, kernel_size = 4, stride = 1, padding = 0, bias=False)),\n",
    "#             # output (bs,1,1,1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         imgs, labels = inputs\n",
    "\n",
    "#         enc = self.label_emb(labels).view((-1, 1, 64, 64))\n",
    "#         enc = F.normalize(enc, p=2, dim=1)\n",
    "#         x = torch.cat((imgs, enc), 1)   # 4 input feature maps(3rgb + 1label)\n",
    "        \n",
    "#         out = self.model(x)\n",
    "#         return out.view(-1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=120, channels=3, nfilt=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.nfilt = nfilt\n",
    "\n",
    "        def down_convlayer(n_input, n_output, k_size=4, stride=2, padding=0, dropout_p=0.0):\n",
    "            block = [spectral_norm(nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)),\n",
    "                     nn.BatchNorm2d(n_output),\n",
    "                     nn.LeakyReLU(0.2, inplace=True),\n",
    "                    ]\n",
    "            if dropout_p > 0.0: block.append(nn.Dropout(p=dropout_p))\n",
    "            return block\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, 64*64)\n",
    "        self.model = nn.Sequential(\n",
    "            *down_convlayer(self.channels + 1, nfilt, k_size = 4, stride = 2, padding = 1),\n",
    "            # output (bs,64,32,32)\n",
    "            Self_Attn(nfilt),\n",
    "            \n",
    "            *down_convlayer(nfilt, nfilt*2, k_size = 4, stride = 2, padding = 1, dropout_p=0.05),\n",
    "            # output (bs,128,16,16)\n",
    "            *down_convlayer(nfilt*2, nfilt*4, k_size = 4, stride = 2, padding = 1, dropout_p=0.10),\n",
    "            # output (bs,256,8,8)\n",
    "            *down_convlayer(nfilt*4, nfilt*8, k_size = 4, stride = 2, padding = 1, dropout_p=0.15),\n",
    "            # output (bs,512,4,4)\n",
    "    \n",
    "            spectral_norm(nn.Conv2d(nfilt*8, out_channels = nfilt*8, kernel_size = 4, stride = 1, padding = 0, bias=False)),\n",
    "            # output (bs,1,1,1)\n",
    "        )\n",
    "        self.top = nn.Sequential(*[spectral_norm(nn.Linear(nfilt*8,nfilt*8)),nn.LeakyReLU(),nn.Linear(nfilt*8,1)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        imgs, labels = inputs\n",
    "\n",
    "        enc = self.label_emb(labels).view((-1, 1, 64, 64))\n",
    "        enc = F.normalize(enc, p=2, dim=1)\n",
    "        x = torch.cat((imgs, enc), 1)   # 4 input feature maps(3rgb + 1label)\n",
    "        \n",
    "        out = self.model(x)\n",
    "        out = self.top(out.view(-1,self.nfilt*8))\n",
    "        return out.view(-1)\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAB = True\n",
    "\n",
    "LAB_DATA = \"../input/generative-dog-images/\"\n",
    "SUBMIT_DATA = \"../input/\"\n",
    "\n",
    "DATA  = LAB_DATA if LAB else SUBMIT_DATA\n",
    "ANNOTATION = DATA+\"annotation/Annotation/\"\n",
    "IMG_DATA = DATA+\"all-dogs/all-dogs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self, directory, transform=None, n_samples=np.inf, crop_dogs=True):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.n_samples = n_samples        \n",
    "        self.samples, self.labels = self.load_dogs_data(directory, crop_dogs)\n",
    "\n",
    "    def load_dogs_data(self, directory, crop_dogs):\n",
    "        required_transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(64),\n",
    "                torchvision.transforms.CenterCrop(64),\n",
    "        ])\n",
    "\n",
    "        imgs = []\n",
    "        labels = []\n",
    "        paths = []\n",
    "        for root, _, fnames in sorted(os.walk(directory)):\n",
    "            for fname in sorted(fnames)[:min(self.n_samples, 999999999999999)]:\n",
    "                path = os.path.join(root, fname)\n",
    "                paths.append(path)\n",
    "        print(\"=\"*60)\n",
    "        print(\"Start Loading Annotation\")\n",
    "        print(\"=\"*60)\n",
    "        t = tqdm(range(len(paths)))\n",
    "        for i in t:\n",
    "            path = paths[i]\n",
    "            # Load image\n",
    "            try: img = dset.folder.default_loader(path)\n",
    "            except: continue\n",
    "            \n",
    "            # Get bounding boxes\n",
    "            annotation_basename = os.path.splitext(os.path.basename(path))[0]\n",
    "            annotation_dirname = next(\n",
    "                    dirname for dirname in os.listdir(ANNOTATION) if\n",
    "                    dirname.startswith(annotation_basename.split('_')[0]))\n",
    "                \n",
    "            if crop_dogs:\n",
    "                tree = ET.parse(os.path.join(ANNOTATION,\n",
    "                                             annotation_dirname, annotation_basename))\n",
    "                root = tree.getroot()\n",
    "                objects = root.findall('object')\n",
    "                for o in objects:\n",
    "                    bndbox = o.find('bndbox')\n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    object_img = required_transforms(img.crop((xmin, ymin, xmax, ymax)))\n",
    "                    imgs.append(object_img)\n",
    "                    labels.append(annotation_dirname.split('-')[1].lower())\n",
    "\n",
    "            else:\n",
    "                object_img = required_transforms(img)\n",
    "                imgs.append(object_img)\n",
    "                labels.append(annotation_dirname.split('-')[1].lower())\n",
    "            \n",
    "        return imgs, labels\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform is not None: \n",
    "            sample = self.transform(sample)\n",
    "        return np.asarray(sample), label\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "database = IMG_DATA\n",
    "crop_dogs = True\n",
    "n_samples = np.inf\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "beta1 = 0.5\n",
    "epochs = 80\n",
    "\n",
    "use_soft_noisy_labels=True\n",
    "\n",
    "nz = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# loading data, takes time\n",
    "train_data = DataGenerator(database, transform=transform, n_samples=n_samples, crop_dogs=crop_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog breeds loaded:   120\n",
      "Data samples loaded: 22125\n"
     ]
    }
   ],
   "source": [
    "decoded_dog_labels = {i:breed for i, breed in enumerate(sorted(set(train_data.labels)))}\n",
    "encoded_dog_labels = {breed:i for i, breed in enumerate(sorted(set(train_data.labels)))}\n",
    "train_data.labels = [encoded_dog_labels[l] for l in train_data.labels] # encode dog labels in the data generator\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True,\n",
    "                                           batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "\n",
    "print(\"Dog breeds loaded:  \", len(encoded_dog_labels))\n",
    "print(\"Data samples loaded:\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator parameters:     15837960\n",
      "Discriminator parameters: 7712754\n"
     ]
    }
   ],
   "source": [
    "netG = Generator(nz, num_classes=len(encoded_dog_labels), nfilt=64).to(device)\n",
    "netD = Discriminator(num_classes=len(encoded_dog_labels), nfilt=64).to(device)\n",
    "weights_init(netG)\n",
    "weights_init(netD)\n",
    "print(\"Generator parameters:    \", sum(p.numel() for p in netG.parameters() if p.requires_grad))\n",
    "print(\"Discriminator parameters:\", sum(p.numel() for p in netD.parameters() if p.requires_grad))\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0010, betas=(beta1, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0005, betas=(beta1, 0.999))\n",
    "\n",
    "lr_schedulerG = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizerG, T_0=epochs//20, eta_min=0.00001)\n",
    "lr_schedulerD = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizerD, T_0=epochs//20, eta_min=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(imageA, imageB):\n",
    "        err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "        err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "        return err\n",
    "\n",
    "def show_generated_img(n_images=5, nz=128):\n",
    "    sample = []\n",
    "    for _ in range(n_images):\n",
    "        noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "        dog_label = torch.randint(0, len(encoded_dog_labels), (1, ), device=device)\n",
    "        gen_image = netG((noise, dog_label)).to(\"cpu\").clone().detach().squeeze(0)\n",
    "        gen_image = gen_image.numpy().transpose(1, 2, 0)\n",
    "        sample.append(gen_image)\n",
    "        \n",
    "    figure, axes = plt.subplots(1, len(sample), figsize=(64, 64))\n",
    "    for index, axis in enumerate(axes):\n",
    "        axis.axis('off')\n",
    "        image_array = (sample[index] + 1.) / 2.\n",
    "        axis.imshow(image_array)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def analyse_generated_by_class(n_images=5):\n",
    "    good_breeds = []\n",
    "    for l in range(len(decoded_dog_labels)):\n",
    "        sample = []\n",
    "        for _ in range(n_images):\n",
    "            noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "            dog_label = torch.full((1,) , l, device=device, dtype=torch.long)\n",
    "            gen_image = netG((noise, dog_label)).to(\"cpu\").clone().detach().squeeze(0)\n",
    "            gen_image = gen_image.numpy().transpose(1, 2, 0)\n",
    "            sample.append(gen_image)\n",
    "        \n",
    "        d = np.round(np.sum([mse(sample[k], sample[k+1]) for k in range(len(sample)-1)])/n_images, 1)\n",
    "        if d < 1.25: continue  # had mode colapse(discard)\n",
    "            \n",
    "        print(f\"Generated breed({d}): \", decoded_dog_labels[l])\n",
    "        figure, axes = plt.subplots(1, len(sample), figsize=(64, 64))\n",
    "        for index, axis in enumerate(axes):\n",
    "            axis.axis('off')\n",
    "            image_array = (sample[index] + 1.) / 2.\n",
    "            axis.imshow(image_array)\n",
    "        plt.show()\n",
    "        \n",
    "        good_breeds.append(l)\n",
    "    return good_breeds\n",
    "\n",
    "\n",
    "def create_submit(good_breeds):\n",
    "    print(\"Creating submit\")\n",
    "    os.makedirs('../output_images', exist_ok=True)\n",
    "    im_batch_size = 100\n",
    "    n_images = 10000\n",
    "    \n",
    "    all_dog_labels = np.random.choice(good_breeds, size=n_images, replace=True)\n",
    "    for i_batch in range(0, n_images, im_batch_size):\n",
    "        noise = torch.randn(im_batch_size, nz, 1, 1, device=device)\n",
    "        dog_labels = torch.from_numpy(all_dog_labels[i_batch: (i_batch+im_batch_size)]).to(device)\n",
    "        #dog_labels = torch.squeeze(torch.randint(0, len(encoded_dog_labels), (im_batch_size, 1,), device=device))\n",
    "        gen_images = netG((noise, dog_labels))\n",
    "        gen_images = (gen_images.to(\"cpu\").clone().detach() + 1) / 2\n",
    "        for ii, img in enumerate(gen_images):\n",
    "            save_image(gen_images[ii, :, :, :], os.path.join('../output_images', f'image_{i_batch + ii:05d}.png'))\n",
    "            \n",
    "    import shutil\n",
    "    shutil.make_archive('images', 'zip', '../output_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class historyReplay(object):\n",
    "    def __init__(self, bs, current_ratio = .2, history_len = 50):\n",
    "        self.current_ratio = current_ratio\n",
    "        self.counter = 0\n",
    "        self.history_len = history_len\n",
    "        self.bs = bs\n",
    "        self.argslist = []\n",
    "        self.arglen = len(self.argslist)\n",
    "        self.latest_chunk = int(bs*current_ratio)\n",
    "        self.history_chunk = bs-self.latest_chunk\n",
    "        \n",
    "    def __call__(self,*args):\n",
    "        # The 1st input\n",
    "        if self.arglen == 0:\n",
    "            self.argslist = args\n",
    "            self.arglen = len(self.argslist)\n",
    "            return tuple(args) if self.arglen>1 else tuple(args)[0]\n",
    "        else:\n",
    "            stack_size = self.argslist[0].size(0)\n",
    "            # the 2nd ~ the history length\n",
    "            if stack_size<self.bs*self.history_len:\n",
    "                self.argslist = list(torch.cat([args[i],self.argslist[i]],dim=0) for i in range(len(self.argslist)))\n",
    "                self.counter+=1\n",
    "                return tuple(args) if self.arglen>1 else tuple(args)[0]\n",
    "            # above history length\n",
    "            else:\n",
    "                pos = self.counter%self.history_len\n",
    "                start_pos = pos* self.bs\n",
    "                end_pos = (pos+1)*self.bs\n",
    "                slice_ = random.choices(range(self.bs*self.history_len), k = self.history_chunk)\n",
    "                rt = []\n",
    "                for i in range(len(self.argslist)):\n",
    "                    rt.append(torch.cat([args[i][:self.latest_chunk,...],self.argslist[i][slice_,...]],dim=0))\n",
    "                    self.argslist[i][start_pos:end_pos,...] = args[i]\n",
    "                self.counter+=1\n",
    "                return tuple(rt) if self.arglen>1 else tuple(rt)[0]\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "replay = historyReplay(bs = BATCH_SIZE, current_ratio = .7, history_len = 30)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    epoch_time = time.perf_counter()\n",
    "    if time.perf_counter() - kernel_start_time > 31000:\n",
    "            print(\"Time limit reached! Stopping kernel!\"); break\n",
    "\n",
    "    for ii, (real_images, dog_labels) in enumerate(train_loader):\n",
    "        if real_images.shape[0]!= BATCH_SIZE: continue\n",
    "        \n",
    "        if use_soft_noisy_labels:\n",
    "            real_labels = torch.squeeze(torch.empty((BATCH_SIZE, 1), device=device).uniform_(0.55, 0.80))\n",
    "            fake_labels = torch.squeeze(torch.empty((BATCH_SIZE, 1), device=device).uniform_(0.00, 0.10))\n",
    "            for p in np.random.choice(BATCH_SIZE, size=np.random.randint((BATCH_SIZE//8)), replace=False):\n",
    "                real_labels[p], fake_labels[p] = fake_labels[p], real_labels[p] # swap labels\n",
    "        else:\n",
    "            real_labels = torch.full((BATCH_SIZE, 1), 1.0, device=device)\n",
    "            fake_labels = torch.full((BATCH_SIZE, 1), 0.0, deviace=device)\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        netD.zero_grad()\n",
    "\n",
    "        dog_labels = torch.tensor(dog_labels, device=device)\n",
    "        real_images = real_images.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, nz, 1, 1, device=device)\n",
    "        \n",
    "        outputR = netD((real_images, dog_labels))\n",
    "        fake_images = netG((noise, dog_labels))\n",
    "        # historic replay\n",
    "        fake_images_rpy,dog_labels_rpy = replay(fake_images.detach(), dog_labels)\n",
    "\n",
    "        outputF = netD((fake_images_rpy,dog_labels_rpy))\n",
    "        errD = (torch.mean((outputR - torch.mean(outputF) - real_labels) ** 2) + \n",
    "                torch.mean((outputF - torch.mean(outputR) + real_labels) ** 2))/2\n",
    "        errD.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        outputF = netD((fake_images, dog_labels))\n",
    "        errG = (torch.mean((outputR - torch.mean(outputF) + real_labels) ** 2) +\n",
    "                torch.mean((outputF - torch.mean(outputR) - real_labels) ** 2))/2\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        lr_schedulerG.step(epoch)\n",
    "        lr_schedulerD.step(epoch)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print('%.2fs [%d/%d] Loss_D: %.4f Loss_G: %.4f' % (\n",
    "              time.perf_counter()-epoch_time, epoch+1, epochs, errD.item(), errG.item()))\n",
    "        show_generated_img(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise generated results by label and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "good_breeds = analyse_generated_by_class(6)\n",
    "create_submit(good_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip, pickle\n",
    "import tensorflow as tf\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "import urllib\n",
    "import warnings\n",
    "from PIL import Image\n",
    "\n",
    "class KernelEvalException(Exception):\n",
    "    pass\n",
    "\n",
    "model_params = {\n",
    "    'Inception': {\n",
    "        'name': 'Inception', \n",
    "        'imsize': 64,\n",
    "        'output_layer': 'Pretrained_Net/pool_3:0', \n",
    "        'input_layer': 'Pretrained_Net/ExpandDims:0',\n",
    "        'output_shape': 2048,\n",
    "        'cosine_distance_eps': 0.1\n",
    "        }\n",
    "}\n",
    "\n",
    "def create_model_graph(pth):\n",
    "    \"\"\"Creates a graph from saved GraphDef file.\"\"\"\n",
    "    # Creates graph from saved graph_def.pb.\n",
    "    with tf.gfile.FastGFile( pth, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString( f.read())\n",
    "        _ = tf.import_graph_def( graph_def, name='Pretrained_Net')\n",
    "\n",
    "def _get_model_layer(sess, model_name):\n",
    "    # layername = 'Pretrained_Net/final_layer/Mean:0'\n",
    "    layername = model_params[model_name]['output_layer']\n",
    "    layer = sess.graph.get_tensor_by_name(layername)\n",
    "    ops = layer.graph.get_operations()\n",
    "    for op_idx, op in enumerate(ops):\n",
    "        for o in op.outputs:\n",
    "            shape = o.get_shape()\n",
    "            if shape._dims != []:\n",
    "              shape = [s.value for s in shape]\n",
    "              new_shape = []\n",
    "              for j, s in enumerate(shape):\n",
    "                if s == 1 and j == 0:\n",
    "                  new_shape.append(None)\n",
    "                else:\n",
    "                  new_shape.append(s)\n",
    "              o.__dict__['_shape_val'] = tf.TensorShape(new_shape)\n",
    "    return layer\n",
    "\n",
    "def get_activations(images, sess, model_name, batch_size=50, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 256.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the disposable hardware.\n",
    "    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, 2048) that contains the\n",
    "       activations of the given tensor when feeding inception with the query tensor.\n",
    "    \"\"\"\n",
    "    inception_layer = _get_model_layer(sess, model_name)\n",
    "    n_images = images.shape[0]\n",
    "    if batch_size > n_images:\n",
    "        print(\"warning: batch size is bigger than the data size. setting batch size to data size\")\n",
    "        batch_size = n_images\n",
    "    n_batches = n_images//batch_size + 1\n",
    "    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        if verbose:\n",
    "            print(\"\\rPropagating batch %d/%d\" % (i+1, n_batches), end=\"\", flush=True)\n",
    "        start = i*batch_size\n",
    "        if start+batch_size < n_images:\n",
    "            end = start+batch_size\n",
    "        else:\n",
    "            end = n_images\n",
    "                    \n",
    "        batch = images[start:end]\n",
    "        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})\n",
    "        pred_arr[start:end] = pred.reshape(-1,model_params[model_name]['output_shape'])\n",
    "    if verbose:\n",
    "        print(\" done\")\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "# def calculate_memorization_distance(features1, features2):\n",
    "#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')\n",
    "#     neigh.fit(features2) \n",
    "#     d, _ = neigh.kneighbors(features1, return_distance=True)\n",
    "#     print('d.shape=',d.shape)\n",
    "#     return np.mean(d)\n",
    "\n",
    "def normalize_rows(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    function that normalizes each row of the matrix x to have unit length.\n",
    "\n",
    "    Args:\n",
    "     ``x``: A numpy matrix of shape (n, m)\n",
    "\n",
    "    Returns:\n",
    "     ``x``: The normalized (by row) numpy matrix.\n",
    "    \"\"\"\n",
    "    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def cosine_distance(features1, features2):\n",
    "    # print('rows of zeros in features1 = ',sum(np.sum(features1, axis=1) == 0))\n",
    "    # print('rows of zeros in features2 = ',sum(np.sum(features2, axis=1) == 0))\n",
    "    features1_nozero = features1[np.sum(features1, axis=1) != 0]\n",
    "    features2_nozero = features2[np.sum(features2, axis=1) != 0]\n",
    "    norm_f1 = normalize_rows(features1_nozero)\n",
    "    norm_f2 = normalize_rows(features2_nozero)\n",
    "\n",
    "    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))\n",
    "    print('d.shape=',d.shape)\n",
    "    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)\n",
    "    mean_min_d = np.mean(np.min(d, axis=1))\n",
    "    print('distance=',mean_min_d)\n",
    "    return mean_min_d\n",
    "\n",
    "\n",
    "def distance_thresholding(d, eps):\n",
    "    if d < eps:\n",
    "        return d\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "            \n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        warnings.warn(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        # covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    \n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # covmean = tf.linalg.sqrtm(tf.linalg.matmul(sigma1,sigma2))\n",
    "\n",
    "    print('covmean.shape=',covmean.shape)\n",
    "    # tr_covmean = tf.linalg.trace(covmean)\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "    # return diff.dot(diff) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(images, sess, model_name, batch_size=50, verbose=False):\n",
    "    \"\"\"Calculation of the statistics used by the FID.\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 255.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the available hardware.\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    \"\"\"\n",
    "    act = get_activations(images, sess, model_name, batch_size, verbose)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma, act\n",
    "    \n",
    "def _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):\n",
    "    path = pathlib.Path(path)\n",
    "    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
    "    imsize = model_params[model_name]['imsize']\n",
    "\n",
    "    # In production we don't resize input images. This is just for demo purpose. \n",
    "    x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n",
    "    m, s, features = calculate_activation_statistics(x, sess, model_name)\n",
    "    del x #clean up memory\n",
    "    return m, s, features\n",
    "\n",
    "# check for image size\n",
    "def img_read_checks(filename, resize_to, is_checksize=False, check_imsize = 64, is_check_png = False):\n",
    "    im = Image.open(str(filename))\n",
    "    if is_checksize and im.size != (check_imsize,check_imsize):\n",
    "        raise KernelEvalException('The images are not of size '+str(check_imsize))\n",
    "    \n",
    "    if is_check_png and im.format != 'PNG':\n",
    "        raise KernelEvalException('Only PNG images should be submitted.')\n",
    "\n",
    "    if resize_to is None:\n",
    "        return im\n",
    "    else:\n",
    "        return im.resize((resize_to,resize_to),Image.ANTIALIAS)\n",
    "\n",
    "def calculate_kid_given_paths(paths, model_name, model_path, feature_path=None):\n",
    "    ''' Calculates the KID of two paths. '''\n",
    "    tf.reset_default_graph()\n",
    "    create_model_graph(str(model_path))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = True)\n",
    "        if feature_path is None:\n",
    "            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)\n",
    "        else:\n",
    "            with np.load(feature_path) as f:\n",
    "                m2, s2, features2 = f['m'], f['s'], f['features']\n",
    "\n",
    "        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))\n",
    "        print('starting calculating FID')\n",
    "        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
    "        print('done with FID, starting distance calculation')\n",
    "        distance = cosine_distance(features1, features2)        \n",
    "        return fid_value, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeLB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip, pickle\n",
    "import tensorflow as tf\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "import urllib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "class KernelEvalException(Exception):\n",
    "    pass\n",
    "\n",
    "model_params = {\n",
    "    'Inception': {\n",
    "        'name': 'Inception', \n",
    "        'imsize': 64,\n",
    "        'output_layer': 'Pretrained_Net/pool_3:0', \n",
    "        'input_layer': 'Pretrained_Net/ExpandDims:0',\n",
    "        'output_shape': 2048,\n",
    "        'cosine_distance_eps': 0.1\n",
    "        }\n",
    "}\n",
    "\n",
    "def create_model_graph(pth):\n",
    "    \"\"\"Creates a graph from saved GraphDef file.\"\"\"\n",
    "    # Creates graph from saved graph_def.pb.\n",
    "    with tf.gfile.FastGFile( pth, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString( f.read())\n",
    "        _ = tf.import_graph_def( graph_def, name='Pretrained_Net')\n",
    "\n",
    "def _get_model_layer(sess, model_name):\n",
    "    # layername = 'Pretrained_Net/final_layer/Mean:0'\n",
    "    layername = model_params[model_name]['output_layer']\n",
    "    layer = sess.graph.get_tensor_by_name(layername)\n",
    "    ops = layer.graph.get_operations()\n",
    "    for op_idx, op in enumerate(ops):\n",
    "        for o in op.outputs:\n",
    "            shape = o.get_shape()\n",
    "            if shape._dims != []:\n",
    "              shape = [s.value for s in shape]\n",
    "              new_shape = []\n",
    "              for j, s in enumerate(shape):\n",
    "                if s == 1 and j == 0:\n",
    "                  new_shape.append(None)\n",
    "                else:\n",
    "                  new_shape.append(s)\n",
    "              o.__dict__['_shape_val'] = tf.TensorShape(new_shape)\n",
    "    return layer\n",
    "\n",
    "def get_activations(images, sess, model_name, batch_size=50, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 256.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the disposable hardware.\n",
    "    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, 2048) that contains the\n",
    "       activations of the given tensor when feeding inception with the query tensor.\n",
    "    \"\"\"\n",
    "    inception_layer = _get_model_layer(sess, model_name)\n",
    "    n_images = images.shape[0]\n",
    "    if batch_size > n_images:\n",
    "        print(\"warning: batch size is bigger than the data size. setting batch size to data size\")\n",
    "        batch_size = n_images\n",
    "    n_batches = n_images//batch_size + 1\n",
    "    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        if verbose:\n",
    "            print(\"\\rPropagating batch %d/%d\" % (i+1, n_batches), end=\"\", flush=True)\n",
    "        start = i*batch_size\n",
    "        if start+batch_size < n_images:\n",
    "            end = start+batch_size\n",
    "        else:\n",
    "            end = n_images\n",
    "                    \n",
    "        batch = images[start:end]\n",
    "        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})\n",
    "        pred_arr[start:end] = pred.reshape(-1,model_params[model_name]['output_shape'])\n",
    "    if verbose:\n",
    "        print(\" done\")\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "# def calculate_memorization_distance(features1, features2):\n",
    "#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')\n",
    "#     neigh.fit(features2) \n",
    "#     d, _ = neigh.kneighbors(features1, return_distance=True)\n",
    "#     print('d.shape=',d.shape)\n",
    "#     return np.mean(d)\n",
    "\n",
    "def normalize_rows(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    function that normalizes each row of the matrix x to have unit length.\n",
    "\n",
    "    Args:\n",
    "     ``x``: A numpy matrix of shape (n, m)\n",
    "\n",
    "    Returns:\n",
    "     ``x``: The normalized (by row) numpy matrix.\n",
    "    \"\"\"\n",
    "    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def cosine_distance(features1, features2):\n",
    "    # print('rows of zeros in features1 = ',sum(np.sum(features1, axis=1) == 0))\n",
    "    # print('rows of zeros in features2 = ',sum(np.sum(features2, axis=1) == 0))\n",
    "    features1_nozero = features1[np.sum(features1, axis=1) != 0]\n",
    "    features2_nozero = features2[np.sum(features2, axis=1) != 0]\n",
    "    norm_f1 = normalize_rows(features1_nozero)\n",
    "    norm_f2 = normalize_rows(features2_nozero)\n",
    "\n",
    "    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))\n",
    "    print('d.shape=',d.shape)\n",
    "    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)\n",
    "    mean_min_d = np.mean(np.min(d, axis=1))\n",
    "    print('distance=',mean_min_d)\n",
    "    return mean_min_d\n",
    "\n",
    "\n",
    "def distance_thresholding(d, eps):\n",
    "    if d < eps:\n",
    "        return d\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "            \n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        warnings.warn(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        # covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    \n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # covmean = tf.linalg.sqrtm(tf.linalg.matmul(sigma1,sigma2))\n",
    "\n",
    "    print('covmean.shape=',covmean.shape)\n",
    "    # tr_covmean = tf.linalg.trace(covmean)\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "    # return diff.dot(diff) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(images, sess, model_name, batch_size=50, verbose=False):\n",
    "    \"\"\"Calculation of the statistics used by the FID.\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 255.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the available hardware.\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    \"\"\"\n",
    "    act = get_activations(images, sess, model_name, batch_size, verbose)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma, act\n",
    "    \n",
    "def _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):\n",
    "    path = pathlib.Path(path)\n",
    "    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
    "    imsize = model_params[model_name]['imsize']\n",
    "\n",
    "    # In production we don't resize input images. This is just for demo purpose. \n",
    "    x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n",
    "    m, s, features = calculate_activation_statistics(x, sess, model_name)\n",
    "    del x #clean up memory\n",
    "    return m, s, features\n",
    "\n",
    "# check for image size\n",
    "def img_read_checks(filename, resize_to, is_checksize=False, check_imsize = 64, is_check_png = False):\n",
    "    im = Image.open(str(filename))\n",
    "    if is_checksize and im.size != (check_imsize,check_imsize):\n",
    "        raise KernelEvalException('The images are not of size '+str(check_imsize))\n",
    "    \n",
    "    if is_check_png and im.format != 'PNG':\n",
    "        raise KernelEvalException('Only PNG images should be submitted.')\n",
    "\n",
    "    if resize_to is None:\n",
    "        return im\n",
    "    else:\n",
    "        return im.resize((resize_to,resize_to),Image.ANTIALIAS)\n",
    "\n",
    "def calculate_kid_given_paths(paths, model_name, model_path, feature_path=None, mm=[], ss=[], ff=[]):\n",
    "    ''' Calculates the KID of two paths. '''\n",
    "    tf.reset_default_graph()\n",
    "    create_model_graph(str(model_path))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = True)\n",
    "        if len(mm) != 0:\n",
    "            m2 = mm\n",
    "            s2 = ss\n",
    "            features2 = ff\n",
    "        elif feature_path is None:\n",
    "            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)\n",
    "        else:\n",
    "            with np.load(feature_path) as f:\n",
    "                m2, s2, features2 = f['m'], f['s'], f['features']\n",
    "\n",
    "        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))\n",
    "        print('starting calculating FID')\n",
    "        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
    "        print('done with FID, starting distance calculation')\n",
    "        distance = cosine_distance(features1, features2)        \n",
    "        return fid_value, distance, m2, s2, features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 201/201 [00:35<00:00,  5.69it/s]\n",
      "100%|| 412/412 [01:11<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1,m2 shape= ((2048,), (2048,)) s1,s2= ((2048, 2048), (2048, 2048))\n",
      "starting calculating FID\n",
      "covmean.shape= (2048, 2048)\n",
      "done with FID, starting distance calculation\n",
      "d.shape= (10000, 20579)\n",
      "np.min(d, axis=1).shape= (10000,)\n",
      "distance= 0.2620773676159945\n",
      "FID_public:  110.00614330017885 distance_public:  1 multiplied_public:  110.00614330017775\n"
     ]
    }
   ],
   "source": [
    "if LAB:\n",
    "  \n",
    "    # UNCOMPRESS OUR IMGAES\n",
    "    with zipfile.ZipFile(\"../working/images.zip\",\"r\") as z:\n",
    "        z.extractall(\"../tmp/images2/\")\n",
    "\n",
    "    # COMPUTE LB SCORE\n",
    "    m2 = []; s2 =[]; f2 = []\n",
    "    user_images_unzipped_path = '../tmp/images2/'\n",
    "    images_path = [user_images_unzipped_path,'../input/generative-dog-images/all-dogs/all-dogs/']\n",
    "    public_path = '../input/dog-face-generation-competition-kid-metric-input/classify_image_graph_def.pb'\n",
    "\n",
    "    fid_epsilon = 10e-15\n",
    "\n",
    "    fid_value_public, distance_public, m2, s2, f2 = calculate_kid_given_paths(images_path, 'Inception', public_path, mm=m2, ss=s2, ff=f2)\n",
    "    distance_public = distance_thresholding(distance_public, model_params['Inception']['cosine_distance_eps'])\n",
    "    print(\"FID_public: \", fid_value_public, \"distance_public: \", distance_public, \"multiplied_public: \",\n",
    "            fid_value_public /(distance_public + fid_epsilon))\n",
    "    \n",
    "    # REMOVE FILES TO PREVENT KERNEL ERROR OF TOO MANY FILES\n",
    "    ! rm -r ../tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
